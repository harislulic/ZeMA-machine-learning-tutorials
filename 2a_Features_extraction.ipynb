{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#  2a Features extraction using Best Fourier Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was provided as part of [2_Features_extraction_and_selection.ipynb](2_Features_extraction_and_selection.ipynb) which contains some functions that take some time for execution. This notebook can give better insights in outputs and is easier to follow. For details, please go to:\n",
    "[2_Features_extraction_and_selection.ipynb](2_Features_extraction_and_selection.ipynb) and execute all cells.\n",
    "\n",
    "\n",
    "*Before usage of this notebook, please download folder `Files_for_notebooks.zip` from this link https://github.com/harislulic/ZeMA-machine-learning-tutorials/releases/tag/v0.1.2\n",
    "and store the files in the same folder which is location for this notebook.\n",
    "It is also necessary to install pip in your environment and using this, package PyDynamic:*\n",
    "\n",
    "pip install PyDynamic\n",
    "\n",
    "*In order to see interactive diagrams, write:* \n",
    "\n",
    "pip install ipywidgets\n",
    "\n",
    "jupyter nbextension enable --py widgetsnbextension   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the ZeMa´s methods, data from all sensors is split into training and test data where rows represent cycles and columns points of time signals. It was done in [2_Features_extraction_and_selection.ipynb](2_Features_extraction_and_selection.ipynb).\n",
    "\n",
    "Training data of time signals from all sensors is transformed into frequency domain. For every frequency bin, the mean value of amplitudes of all cycles is calculated. Then, the matrix of amplitudes is sorted from the highest to the lowest mean value. Indices of columns in matrix of uncertainties follow these columns. For simplicity, phases and their uncertainties are neglected, but they should be taken into account in the future.\n",
    "\n",
    "From the matrix of amplitudes, 10% of columns labelled with the frequency bins will be extracted (10% of the spectrum). The same will happen with the matrix of uncertainties. Based on this, feature selection will be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a.2 Machine Learning using Best Fourier Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 2a.2.1 Importing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py                                     # Importing the h5 package.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import PyDynamic  \n",
    "from PyDynamic import __version__ as version\n",
    "from PyDynamic.uncertainty.propagate_DFT import GUM_DFT\n",
    "from PyDynamic.uncertainty.propagate_DFT import DFT2AmpPhase\n",
    "from scipy.sparse import dia_matrix\n",
    "from PyDynamic.uncertainty.propagate_DFT import AmpPhase2Time\n",
    "from PyDynamic.uncertainty.propagate_DFT import AmpPhase2DFT, GUM_iDFT,Time2AmpPhase_multi\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://zenodo.org/record/1326278/files/Sensor_data_2kHz.h5'\n",
    "\n",
    "def get_filename(url):\n",
    "    return url.split('/')[-1]\n",
    "\n",
    "filename = get_filename(url)                    # Data filename.\n",
    "f = h5py.File(filename, 'r')                    # Importing the h5 file. \n",
    "    \n",
    "#print(\"Keys: %s\" % f.keys())\n",
    "a_group_key = list(f.keys())[0]\n",
    "\n",
    "data = list(f[a_group_key])                       # Transforming data into list\n",
    "\n",
    "sensorADC=[]                                      # Initialising a list \"sensor\" and\n",
    "for i in range(len(data)):                        # Filling it with data from all sensors \n",
    "    sensorADC.append(pd.DataFrame(data[i]))\n",
    "\n",
    "for i in range(len(data)):                             \n",
    "    sensorADC[i]=sensorADC[i].iloc[:,:-1]         # Cuting the last cycle because it contains all zero elements.\n",
    "\n",
    "print(\"\"\"    \n",
    "Input matrices have dimensions: %s, where %s represents number of measurements in time\n",
    "and %s represents number of cycles.\"\"\" % (np.shape(sensorADC[0]),np.shape(sensorADC[0])[0],np.shape(sensorADC[0])[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a.2.2 Converting into SI units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset=[0, 0, 0, 0, 0.00488591, 0.00488591, 0.00488591,  0.00488591, 1.36e-2, 1.5e-2, 1.09e-2]\n",
    "gain=[5.36e-9, 5.36e-9, 5.36e-9, 5.36e-9, 3.29e-4, 3.29e-4, 3.29e-4, 3.29e-4, 8.76e-5, 8.68e-5, 8.65e-5]\n",
    "b=[1, 1, 1, 1, 1, 1, 1, 1, 5.299641744, 5.299641744, 5.299641744]\n",
    "k=[250, 9.81, 98.1, 98.1, 1.25, 100000, 30, 0.5, 2, 2, 2]\n",
    "units=['[Pa]', '[mm/s^2]', '[mm/s^2]', '[mm/s^2]', '[kN]', '[Pa]', '[mm/s]', '[A]', '[A]', '[A]', '[A]']\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "sensor = list(sensorADC)\n",
    "for i, df in enumerate(sensor):\n",
    "    for row_label, row in df.iterrows():\n",
    "        sensor[i].iloc[row_label,:]=((sensorADC[i].iloc[row_label,:]*gain[i])+offset[i])*b[i]*k[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sensor=pd.read_csv(r'C:\\Users\\jugo01\\Desktop\\sensor_units.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you have problems with previous step, you can skip conversion into SI units by running next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor=sensorADC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a.2.3 Reading of train and test data\n",
    "*Note: see [2_Features_extraction_and_selection.ipynb](2_Features_extraction_and_selection.ipynb)  Data were split into train and test data for k=85% \n",
    "Based on this, target_train_vector and target_test_vector were provided. These vectors will be used for the Fourier transform into frequency domain.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test1= h5py.File(\"Train_test_data_split\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_vector=train_test1[\"target_train_vector\"]\n",
    "target_test_vector=train_test1[\"target_test_vector\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting arrays into data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_vector=pd.DataFrame(target_train_vector)\n",
    "target_test_vector=pd.DataFrame(target_test_vector)\n",
    "target=list(target_train_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So, after this step main data to work on are lists: \n",
    "\n",
    "\"sensor_train\" with their class labels \"train_target\"\n",
    " \n",
    "and \n",
    " \n",
    "\"sensor_test\" with their class labels \"test_target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_train=[0]*11\n",
    "sensor_test=[0]*11\n",
    "\n",
    "for i in range(11):\n",
    "    sensor_train[i]=sensor[i].loc[:,target_train_vector.index]\n",
    "\n",
    "print(\"Traning data for one sensor has dimensions: \", sensor_train[10].shape,\",      ('sensor_train') \")\n",
    "print(\"and it's target vector has length: \", target_train_vector.shape,\",               ('target_train_vector') \\n\")\n",
    "\n",
    "for i in range(11):\n",
    "    sensor_test[i]=sensor[i].loc[:,target_test_vector.index]\n",
    "\n",
    "print(\"Testing data for one sensor has dimensions: \", sensor_test[10].shape,\",      ('sensor_test') \")\n",
    "print(\"and it's target vector has length: \", target_test_vector.shape,\",        ('target_test_vector') \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at the data from one sensor after splitting for better understanding of structure for next steps. Number of rows is 2000 and each column is one random measurement cycle. Table shows only first five samples in time (five rows) for each cycle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_train[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  2a.2.4 Fast Fourier transform\n",
    "\n",
    "###### Steps:  \n",
    "    \n",
    "- transformation into frequency domain (FFT)\n",
    "- choose amplitudes with highest average absolute value (the top 10%)\n",
    "\n",
    "\n",
    "In this method of feature extraction, data is transformed into frequency domain using FFT function for discrete Fourier transform. More detail about FFT in [1_FFT_and_Reconstruction.ipynb](1_FFT_and_Reconstruction.ipynb)\n",
    "\n",
    "This step is an unsupervised extraction method (i.e. is done without knowledge of the cycle‘s group affiliation) and used is to reduce dimension for further steps.\n",
    "\n",
    "Two functions are provided for this task. First function,`chooseAndReturnOrdered` involves FFT function directly from Numpy library and as output, it gives complex number from which amplitudes are to be calculated. \n",
    "\n",
    "The second function, `chooseAndReturnOrdered_with_uncertainty` is using PyDynamic software and it obtains amplitudes directly, from the function `Time2AmpPhase_multi` with uncertainties as consequence of white noise. It also includes the phases and all of the uncertainties, but computational time is much higher.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A function  `chooseAndReturnOrderedcreated`,  takes as an input: \n",
    "- data from one sensor `sensor`,                                 \n",
    "- number of samples `n_of_samples`,                                    \n",
    "- percentage of data to choose `N`.\n",
    "\n",
    "Function does fast Fourier transform and chooses N% of sprectrum with highest average of absolute values for each sensor independently. Average of absolute values for one frequency is calculated through all cycles.                                   \n",
    "\n",
    "\n",
    "###### Function returns:\n",
    "- `freq_of_sorted_values` matrix sized [1, N% of features (amplitudes)] where elements are frequencies which are choosen and they are labels for second output from this function.\n",
    "- `sorted_values_matrix` sized [number of cycles, N% of features (amplitudes)] where row represents one cycle and columns are sorted by the average of absolute vales for each frequency (column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseAndReturnOrdered(sensor, n_of_samples, N): \n",
    "    x_measurements=range(sensor.shape[0])                 # Number of measurements samples in time period.\n",
    "    x = np.true_divide(x_measurements, n_of_samples)      # Time values, used  as real time axis.\n",
    "    freq = np.fft.rfftfreq(x.size, 0.0005)                # Frequency axis, can be used for ploting in frequency domain.\n",
    "    fft_amplitudes = np.fft.rfft(sensor,n_of_samples,0)   # Ndarray of amplitudes after fourier transform.\n",
    "    fft_matrix = pd.DataFrame(fft_amplitudes)             # Transforming amplitudes into data frame (matrix)-\n",
    "                                                          # -where one column represents amplitudes of one-\n",
    "                                                          # -cycle.\n",
    "    fft_matrix=fft_matrix.transpose()                     # Transposing to matrix where rows are cycles.\n",
    "    n_rows, n_columns = np.shape(fft_matrix)\n",
    "\n",
    "    print(\"\\nNumber of cycles is: %s, and number of features is: %s\" % (n_rows, n_columns))\n",
    "    fft_matrix.columns = freq                    # Column labels are frequencies. \n",
    "    \n",
    "    # Calculating the average of absolute vales for each frequency (column).\n",
    "    absolute_average_values_from_columns=(np.abs(fft_matrix)).mean()\n",
    "    \n",
    "    # Sorting the fft_matrix by the average of absolute vales for each frequency (column).\n",
    "    fft_matrix=fft_matrix.reindex((np.abs(fft_matrix)).mean().sort_values(ascending=False).index, axis=1)\n",
    "    \n",
    "    # Taking first N percent columns from sorted fft_matrix. \n",
    "    sorted_values_matrix=fft_matrix.iloc[:,:round((N/100.0)*len(freq))]\n",
    "    \n",
    "    n_rows, n_columns = np.shape(sorted_values_matrix)\n",
    "    print(\"\\nNumber of cycles is: %s, and number of selected features is: %s\" % (n_rows, n_columns))\n",
    "    print(np.shape(sorted_values_matrix))\n",
    "    \n",
    "    # Informations about the selected frequencies are columns in sorted data frame. \n",
    "    freq_of_sorted_values=(pd.DataFrame(sorted_values_matrix.columns)).transpose()\n",
    "    print(\"\\nFirst 10 selected frequencies are:\\n\\n %s\" % freq_of_sorted_values.values[:,:10])\n",
    "    \n",
    "    sorted_values_matrix.columns=range(round((N/100.0)*len(freq))) # Resetting the column labels.\n",
    "    print(\"---------------------------------------------------------------------------------\\n\")\n",
    "    # Output \"sorted_values_matrix\" is data frame whose rows-\n",
    "    # -are cycles and columns are selected frequencies. For example,- \n",
    "    # -value at position (i,j) is amplitude for frequency j in cycle i.\n",
    "    \n",
    "    return freq_of_sorted_values, sorted_values_matrix;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Function execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instead of executing the function, results of extracting 10% of spectrum with highest average of amplitudes by FFT can be read in the next steps. Values were obtained by using factor of splitting data into train and test from 3a.2.3.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_fft1= h5py.File(\"Sorted_vaules_from_all_sensors.hdf5\",\"r\")\n",
    "freq_fft1= h5py.File(\"Sorted_freq_from_all_sensors.hdf5\",\"r\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_of_sorted_values=[0]*len(sensor_train)\n",
    "sorted_values_from_all_sensors=[0]*len(sensor_train)\n",
    "for i in range(len(sensor)):\n",
    "    freq_of_sorted_values[i]=freq_fft1[\"freq_of_sorted_values\"+str(i)]\n",
    "    sorted_values_from_all_sensors[i]=amp_fft1[\"sorted_values_from_all_sensors\"+str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sensor)):\n",
    "    freq_of_sorted_values[i]=pd.DataFrame( freq_of_sorted_values[i])\n",
    "    sorted_values_from_all_sensors[i]=pd.DataFrame( sorted_values_from_all_sensors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User is asked to define how many of features from frequency domain will be extracted in this step. Then, the function is executed for each sensor and extracted data is stored in 2 lists containing data frames mentioned above. Lists `freq_of_sorted_values` and `sorted_values_from_all_sensors` store function outputs and further selection of features is continued on list `sorted_values_from_all_sensors`. Informations about frequency are going to be used in for feature extraction from testing data, because these frequencies are pattern learned from training data and used for selecting from the testing data or some new data which need to be predicted. \n",
    "For all sensors, selected frequencies with most dominant amplitudes are listed as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_samples=np.shape(sensor_train[0])[0]\n",
    "\n",
    "N = int(input(\"Optimal and recommended percentage of features for this dataset is 10. \\n\\nEnter a percentage of features: \"))\n",
    "print(\"\\n\\n\")\n",
    "# Initialising the list woth 11 elements, which are data frames \"sorted_value_matrix\" from each sensor.\n",
    "freq_of_sorted_values=[0]*len(sensor_train)\n",
    "sorted_values_from_all_sensors=[0]*len(sensor_train)\n",
    "\n",
    "for i in range(len(sensor_train)):                     \n",
    "    print(\"Sensor number %s\" % i)\n",
    "    print(\"---------------------------------------------------------------------------------\")\n",
    "    freq_of_sorted_values[i],sorted_values_from_all_sensors[i]=chooseAndReturnOrdered(sensor_train[i], n_of_samples, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features are complex numbers, because output from the Fourier transform is resulting with amplitudes and phase shifts. For methods used here, amplitudes are more important than phase shifts, and features that will be used are absolute values of amplitudes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Example of frequency labels for features extracted from microphone with the best Fourier coefficients method._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example. We will take first sensor, which is microphone, as an example. The most dominant frequancy for microphone is 0 Hz, and then 480 Hz. That can be seen in `freq_of_sorted_values[0]`. First two columns in `sorted_values_from_all_sensors[0]` are  amplitudes through all measurement cycles for frequencies 0 and 480 Hz, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_of_sorted_values[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.2.4.1 Fast Fourier transform performed with PyDynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the PyDynamic´s function `Time2AmpPhase_multi`  the following is conducted:\n",
    "    \n",
    "- transformation into frequency domain (DFT) with uncertainty propagation \n",
    "[1_DFT_and_uncertainty_propagation.ipynb](1_DFT_and_uncertainty_propagation.ipynb)\n",
    "- extraction of amplitudes with highest average absolute value (the top 10%), with corresponding phases and uncertainties.\n",
    "\n",
    "Function `Time2AmpPhase_multi` returns diagonal elements of matrix of uncertainties in the form of matrix (M,3*N*), where *M* represents the number of cycles and *N* the number of frequencies in frequency domain. It is multiplied by 3, because it contains (M,N) uncertainties of amplitudes, (M,N) of covariance uncertainties between amplitudes and phases, and (M,N) uncertainties of phases. It can be imagined as block of three matrices in this sequence. This is done because of the memory issuses.\n",
    "\n",
    "###### Function returns:\n",
    "- ` A` np.ndarray sized (M,N) where elements are amplitudes of time domain signals of length N in frequency domain for M cycles. \n",
    "- ` P` np.ndarray sized (M,N) where elements are phases of time domain signals of length N in frequency domain for M cycles. \n",
    "-  `UAP`np.ndarray sized (M, 3N) where elements are squared standard uncertainties of amplitudes and phases and their covariances for M cycles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amplitudes are sorted through all cycles and  10% of the spectrum with the highest ones was extracted. Phases and uncertainties are sorted in the way to follow the sorting method for amplitudes. \n",
    "\n",
    "###### Function `chooseAndReturnOrdered_with_uncertainty` returns:\n",
    "- `freq_of_sorted_values` matrix sized [1, N% of spectrum] where elements are frequencies which are choosen and they are labels for second output from this function. They will be the same as in section 3a.2.4.\n",
    "- `sorted_values_amp` sized [number of cycles, N% of spectrum (amplitudes)] where row represents one cycle and columns are sorted by the average of absolute values of amplitudes for each frequency (column). They will be the same as in section 3a.2.4.\n",
    "- `sorted_values_phases` - sized [number of cycles, N% of spectrum (phases)] where row represents one cycle and columns of phases are sorted by chosen frequencies. \n",
    "- `sorted_values_uncert_aa` - sized [number of cycles, N% of spectrum (uncertainties)] where row represents one cycle and columns of squared standard uncertainties for amplitudes are sorted by chosen frequencies. \n",
    "- `sorted_values_uncert_ap` - sized [number of cycles, N% of spectrum (uncertainties)] where row represents one cycle and columns of covariances for amplitudes and phases are sorted by chosen frequencies. \n",
    "- `sorted_values_uncert_pp`  - sized [number of cycles, N% of spectrum (uncertainties)] where row represents one cycle and columns of squared standard uncertainties for phases are sorted by chosen frequencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseAndReturnOrdered_with_uncertainty(sensor, n_of_samples, N,sigma):\n",
    "    \n",
    "    x_measurements=range(sensor.shape[0])        \n",
    "    n_of_samples=np.shape(sensor)[0]                            # number of sampling points\n",
    "    x = np.true_divide(x_measurements, n_of_samples)            # time steps \n",
    "    freq=PyDynamic.uncertainty.propagate_DFT.GUM_DFTfreq(x.size, 0.0005)\n",
    "    ux=np.ones(sensor.shape[1])*sigma**2\n",
    "    a,b,c=Time2AmpPhase_multi(sensor.values.transpose(),ux)\n",
    "    a=pd.DataFrame(a) #amplitudes (M,N)\n",
    "    b=pd.DataFrame(b) #phases (M,N)\n",
    "    c=pd.DataFrame(c) #uncertainties (M,3*N)\n",
    "    a.columns = freq                    # Column labels are frequencies. \n",
    "    n_rows, n_columns=a.shape\n",
    "    print(\"\\nNumber of cycles is: %s, and number of features is: %s\" % (n_rows, n_columns))\n",
    "    # Calculating the average of absolute vales for each frequency (column).\n",
    "    absolute_average_values_from_columns=np.abs(a.mean())\n",
    "    # Sorting column indices in amplitudes for sorting phases and uncertainties\n",
    "    sorted_columns=np.argsort(absolute_average_values_from_columns)[::-1]\n",
    "    # Uncertaintites have one dimension larger than amplitudes and phases. # Columns indices(len(a):3*len(a) to follow the sorting)\n",
    "    sorted_columns_unc_ap=(sorted_columns+a.shape[1])\n",
    "    sorted_columns_unc_pp=(sorted_columns+a.shape[1]*2)\n",
    "    sorted_columns_unc=np.concatenate((sorted_columns,sorted_columns_unc_ap,sorted_columns_unc_pp))#sorted indices for uncertainties\n",
    "    # Reindexing all matrices based on columns.\n",
    "    a=a.reindex((np.abs(a)).mean().sort_values(ascending=False).index, axis=1)\n",
    "    b=b.reindex(columns=sorted_columns)\n",
    "    c=c.reindex(columns=sorted_columns_unc)\n",
    "    # Taking first N percent columns from sorted amplitudes,phases and ucertainties. \n",
    "    sorted_values_amp=a.iloc[:,:round((N/100.0)*len(freq))]\n",
    "    sorted_values_phases=b.iloc[:,:round((N/100.0)*len(freq))] \n",
    "    sorted_values_uncert_aa=c.iloc[:,:round((N/100.0)*len(freq))]\n",
    "    sorted_values_uncert_ap=c.iloc[:,len(freq):a.shape[1]+round((N/100.0)*len(freq))]\n",
    "    sorted_values_uncert_pp=c.iloc[:,2*len(freq):a.shape[1]*2+round((N/100.0)*len(freq))]                                             \n",
    "    n_rows, n_columns = np.shape(sorted_values_amp)\n",
    "    print(\"\\nNumber of cycles is: %s, and number of selected features is: %s\" % (n_rows, n_columns))\n",
    "    print(np.shape(sorted_values_amp))\n",
    "    \n",
    "    # Informations about the selected frequencies are columns in sorted data frame. \n",
    "    freq_of_sorted_values=(pd.DataFrame(sorted_values_amp.columns)).transpose()\n",
    "    print(\"\\nFirst 10 selected frequencies are:\\n\\n %s\" % freq_of_sorted_values.values[:,:10])\n",
    "    \n",
    "    # Resetting the column labels.\n",
    "    sorted_values_amp.columns=range(round((N/100.0)*len(freq)))\n",
    "    sorted_values_phases.columns=range(round((N/100.0)*len(freq)))\n",
    "    sorted_values_uncert_aa.columns=range(round((N/100.0)*len(freq)))\n",
    "    sorted_values_uncert_ap.columns=range(round((N/100.0)*len(freq)))\n",
    "    sorted_values_uncert_pp.columns=range(round((N/100.0)*len(freq)))\n",
    "\n",
    "    print(\"---------------------------------------------------------------------------------\\n\")\n",
    "    # Output \"sorted_values_matrix\" is data frame whose rows-\n",
    "    # -are cycles and columns are selected frequencies. For example,- \n",
    "    # -value at position (i,j) is amplitude for frequency j in cycle i.\n",
    "    return freq_of_sorted_values,sorted_values_amp,sorted_values_phases,sorted_values_uncert_aa,sorted_values_uncert_ap, sorted_values_uncert_pp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Function execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User is asked to define how many of features from frequency domain will be extracted in this step. Then, the function is executed for each sensor and extracted data is stored in 6 lists containing data frames mentioned above. Lists:\n",
    "- freq_of_sorted_values\n",
    "- sorted_values_amp_from_all_sensors \n",
    "- sorted_phases_from_all_sensors\n",
    "- sorted_uncer_from_all_sensors_a\n",
    "- sorted_uncer_from_all_sensors_ap\n",
    "- sorted_uncer_from_all_sensors_pp\n",
    "\n",
    "store function outputs and further selection of features is continued  through loop.\n",
    "\n",
    "*Note: the function for 11 sensors takes approx. 2 hours to execute. Instead of executing the function, results of extracting 10% of highest amplitudes by DFT can be read in the next steps. Values were obtained by using factor of splitting data into train and test from the above. In that case, skip the step \"Function execution\".  $\\sigma$ , representing white noise was assumed as 0.1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_dft2= h5py.File(\"DFTSorted_vaules__from_all_sensors.hdf5\",\"r\")\n",
    "freq_dft2= h5py.File(\"DFTSorted_freq_from_all_sensors.hdf5\",\"r\") \n",
    "ph_dft2= h5py.File(\"DFTSorted_ph_from_all_sensors.hdf5\",\"r\")\n",
    "u_a_dft2= h5py.File(\"DFTSorted_uncer_from_all_sensors_a.hdf5\",\"r\")\n",
    "u_ap_dft2= h5py.File(\"DFTSorted_uncer_from_all_sensors_ap.hdf5\",\"r\")    \n",
    "u_pp_dft2= h5py.File(\"DFTSorted_uncer_from_all_sensors_pp.hdf5\",\"r\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_of_sorted_values=[0]*len(sensor_train)\n",
    "sorted_values_amp_from_all_sensors=[0]*len(sensor_train)\n",
    "sorted_phases_from_all_sensors=[0]*len(sensor_train)\n",
    "sorted_uncer_from_all_sensors_a=[0]*len(sensor_train)\n",
    "sorted_uncer_from_all_sensors_ap=[0]*len(sensor_train)\n",
    "sorted_uncer_from_all_sensors_pp=[0]*len(sensor_train)\n",
    "for i in range(len(sensor)):\n",
    "    freq_of_sorted_values[i]=freq_dft2[\"freq_of_sorted_values\"+str(i)]\n",
    "    sorted_values_amp_from_all_sensors[i]=amp_dft2[\"sorted_values_amp_from_all_sensors\"+str(i)]\n",
    "    sorted_phases_from_all_sensors[i]=ph_dft2[\"sorted_phases_from_all_sensors\"+str(i)]\n",
    "    sorted_uncer_from_all_sensors_a[i]=u_a_dft2[\"sorted_uncer_from_all_sensors_a\"+str(i)]\n",
    "    sorted_uncer_from_all_sensors_ap[i]=u_ap_dft2[\"sorted_uncer_from_all_sensors_ap\"+str(i)]\n",
    "    sorted_uncer_from_all_sensors_pp[i]=u_pp_dft2[\"sorted_uncer_from_all_sensors_pp\"+str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sensor)):\n",
    "    freq_of_sorted_values[i]=pd.DataFrame(freq_of_sorted_values[i])\n",
    "    sorted_values_amp_from_all_sensors[i]=pd.DataFrame(sorted_values_amp_from_all_sensors[i])\n",
    "    sorted_phases_from_all_sensors[i]=pd.DataFrame(sorted_phases_from_all_sensors[i])\n",
    "    sorted_uncer_from_all_sensors_a[i]=pd.DataFrame(sorted_uncer_from_all_sensors_a[i])\n",
    "    sorted_uncer_from_all_sensors_ap[i]=pd.DataFrame(sorted_uncer_from_all_sensors_ap[i])\n",
    "    sorted_uncer_from_all_sensors_pp[i]=pd.DataFrame(sorted_uncer_from_all_sensors_pp[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still want to execute the function, it can be done here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A white noise will be added to the signals in *sensor_train*:\n",
    "\n",
    "$$x_{n}(t) = x(t)+\\epsilon$$\n",
    "\n",
    "White noise has normal distribution ${\\mathcal {N}}(0 ,\\sigma ^{2})$ and standard deviation that can be specified by user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function execution\n",
    "n_of_samples=np.shape(sensor_train[0])[0]\n",
    "\n",
    "N = int(input(\"Optimal and recommended percentage of features for this dataset is 10. \\n\\nEnter a percentage of features: \"))\n",
    "print(\"\\n\\n\")\n",
    "sigma=float(input(\"Assume standard deviation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you did not convert the signal to SI units, otherwise, skip\n",
    "offset=[0, 0, 0, 0, 0.00488591, 0.00488591, 0.00488591,  0.00488591, 1.36e-2, 1.5e-2, 1.09e-2]\n",
    "gain=[5.36e-9, 5.36e-9, 5.36e-9, 5.36e-9, 3.29e-4, 3.29e-4, 3.29e-4, 3.29e-4, 8.76e-5, 8.68e-5, 8.65e-5]\n",
    "b=[1, 1, 1, 1, 1, 1, 1, 1, 5.299641744, 5.299641744, 5.299641744]\n",
    "k=[250, 1, 10, 10, 1.25, 1, 30, 0.5, 2, 2, 2]\n",
    "units=['[Pa]', '[g]', '[g]', '[g]', '[kN]', '[bar]', '[mm/s]', '[A]', '[A]', '[A]', '[A]']\n",
    "\n",
    "for i in range((len(sensor_train))):\n",
    "\n",
    "    sensor_train[i]=((sensor_train[i]*gain[sensor_num])+offset[sensor_num])*b[sensor_num]*k[sensor_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding white noise\n",
    "for i in range((len(sensor_train))):\n",
    "    for k in range((sensor_train[i].shape[1])):\n",
    "           sensor_train[i].iloc[:,k]=sensor_train[i].iloc[:,k]+ np.random.randn(sensor_train[0].shape[0])*sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialising the list woth 11 elements, which are data frames \"sorted_value_matrix\" from each sensor.\n",
    "                    \n",
    "freq_of_sorted_values=[0]*len(sensor_train)\n",
    "sorted_values_amp_from_all_sensors=[0]*len(sensor_train)\n",
    "sorted_phases_from_all_sensors=[0]*len(sensor_train)\n",
    "sorted_uncer_from_all_sensors_a=[0]*len(sensor_train)\n",
    "sorted_uncer_from_all_sensors_ap=[0]*len(sensor_train)\n",
    "sorted_uncer_from_all_sensors_pp=[0]*len(sensor_train)\n",
    "\n",
    "    \n",
    "for i in range(len(sensor_train)):                     \n",
    "    print(\"Sensor number %s\" % i)\n",
    "    print(\"---------------------------------------------------------------------------------\")\n",
    "    freq_of_sorted_values[i],sorted_values_amp_from_all_sensors[i],sorted_phases_from_all_sensors[i],sorted_uncer_from_all_sensors_a[i],sorted_uncer_from_all_sensors_ap[i],sorted_uncer_from_all_sensors_pp[i]=chooseAndReturnOrdered_with_uncertainty(sensor_train[i], n_of_samples, N,sigma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_of_sorted_values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: When amplitudes are small relative to the uncertainty associated with real and imaginary parts , the GUM uncertainty propagation becomes unreliable and a Monte Carlo method is recommended instead. Consequently, GUM2DFT does raise a warning to the user and recommends using a Monte Carlo method instead whenever an element of  is below a pre-defined threshold. The default threshold in GUM2DFT is 1.0, but may be adjusted for specific applications.[7]*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of the results:\n",
    "\n",
    "*Note: Be aware of randomness of splitting data into train and test. Results shown here are for one random split. Functions FFT and DFT were executed for the same split of data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_values_amp_from_all_sensors[0].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_uncer_from_all_sensors_a[0].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_uncer_from_all_sensors_ap[0].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_of_sorted_values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a.2.5 Additional: Transformation to time domain for all sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation from amplitude and phase to time domain is demonstrated with functions `Reconstruct_time_domain` (on the basis of PyDynamic´s function AmpPhase2Time) and `Reconstruct_time_domain_idft`(on the basis of PyDynamic´s functions AmpPhase2DFT, GUM_iDFT).\n",
    "\n",
    "First, zero arrays of amplitudes, phases and uncertainties are created (A, P, UAP). Then, function copies values of N% of arguments (amplitudes, phases, u_a, u_ap, u_pp) into column indices (frequencies) of initial zero arrays. \n",
    "\n",
    "Function `Reconstruct_time_domain`creates matrix of uncertainties with non - zero diagonal elements and all others as zeros, which means that the function can be applied only if UAP is obtained from the signals with the white noise. For each cycle, function returns:  \n",
    "- `x` (np.ndarray ) – vector of time domain values and \n",
    "- `ux` -  (np.ndarray) – standard squared uncertainties of x.\n",
    "Here, ux stores only diagonal elements of UX.\n",
    "\n",
    "*Note: the function for 1 sensors takes approx.1 hour to execute. If you believe in your machine and have enough time, you can do that for all sensors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reconstruct_time_domain(N,frequencies,amplitudes,phases,u_a,u_ap,u_pp):\n",
    "    \n",
    "    M,num=amplitudes.shape\n",
    "    length=int((amplitudes.shape[1])/(((N/100.0))))+1 \n",
    "    length_2=2*length\n",
    "    length_3=3*length \n",
    "    x=np.zeros((M, length_2-2))\n",
    "   #storing uncertainties in a list of arrays\n",
    "    ux=np.zeros((M, length_2-2))\n",
    "    #predefining A,P,UAP as zero arrays\n",
    "    A =np.zeros((M, length))\n",
    "    P= np.zeros((M, length))\n",
    "    UAP=np.zeros((M, 3*length))\n",
    "    assert(amplitudes.shape==phases.shape)\n",
    "    # Indices of columns with highest amplitudes in original matrix (resulted from DFT) are accessible from the\n",
    "    #sorted frequencies of all sensors\n",
    "    Index_amplitudes=frequencies[:,:round((N/100.0)*(length))] \n",
    "    # Defining offsets for sparse matrix  \n",
    "    offset_UAP=[0,length,-length]\n",
    "     #indices(columns) of 10% highest amplitude values\n",
    "    Index_amplitudes=Index_amplitudes.astype(int)\n",
    "    col = np.array(Index_amplitudes[0])\n",
    "    #Values of 10% highest amplitudes(first N columns of input amplitudes) are copied in A,P,UAP in corresponding indices \n",
    "    #of columns. Other columns are zeros. \n",
    "    amp_col=np.arange(round((N/100.0)*(length)))\n",
    "    A[:, [col]]= amplitudes[:, [amp_col]]\n",
    "    P[:, [col]]= phases[:, [amp_col]]\n",
    "    UAP[:,[col]]=u_a[:,[amp_col]]\n",
    "    UAP[:,[col+length]]=u_ap[:,[amp_col]]\n",
    "    UAP[:,[col+length_2]]=u_pp[:,[amp_col]]\n",
    "    for m in range(M): \n",
    "        # Defining diagonals for sparse matrix  \n",
    "        diag1=np.zeros(length_2)\n",
    "        diag2=np.zeros(length_2)\n",
    "        diag1[:length]=UAP[m,:length]\n",
    "        diag1[length:]=UAP[m,length_2:]\n",
    "        diag3=np.zeros(length_2)\n",
    "        diag2[offset_UAP[1]:length_2+offset_UAP[1]]=UAP[m][length:length_2] \n",
    "        diag3[offset_UAP[1]:length_2+offset_UAP[1]]=UAP[m][length:length_2]\n",
    "        diagonals =[diag1,diag2,diag3]\n",
    "        # Creating sparse matrix with three diagonals. Diag1 is the main diagonal.\n",
    "        Sparse_matr=dia_matrix((diagonals,offset_UAP),shape=((length_2, length_2)))\n",
    "        X,UX=AmpPhase2Time(A[m,:], P[m,:], Sparse_matr)\n",
    "        x[m,:] = X\n",
    "        ux[m,:]=np.diag(UX)\n",
    "  \n",
    "    return    x,ux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `Reconstruct_time_domain_idft` gradually performs transformation from amplitudes and phases to real and imaginary parts and then to time domain, taking into account squared standard uncertainties of amplitudes and phases.\n",
    "\n",
    "*Note: the function for 1 sensors takes approx.1 hour to execute. If you believe in your machine and have enough time, you can do that for all sensors.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reconstruct_time_domain_idft(N,frequencies,amplitudes,phases,u_a,u_pp):\n",
    "   \n",
    "    M,num=amplitudes.shape\n",
    "    length=int((amplitudes.shape[1])/(((N/100.0))))+1 \n",
    "    length_2=2*length\n",
    "    x=np.zeros((M, length_2-2))\n",
    "    #storing uncertainties in a list of arrays\n",
    "    ux=np.zeros((M, length_2-2))\n",
    "    #predefining A,P,UAP as zero arrays\n",
    "    A =np.zeros((M, length))\n",
    "    P= np.zeros_like(A)\n",
    "    UAP=np.zeros((M, length_2)) #UAP contains squared standard uncertainties of amplitudes and phases\n",
    "    assert(amplitudes.shape==phases.shape)\n",
    "    # Indices of columns with highest amplitudes\n",
    "    Index_amplitudes=frequencies[:,:round((N/100.0)*(length))] #promijeniti index,generisati\n",
    "    Index_amplitudes=Index_amplitudes.astype(int)\n",
    "    col = np.array(Index_amplitudes[0])\n",
    "    #indices(columns) of 10% highest amplitude values\n",
    "    #first N columns of input amplitudes\n",
    "    amp_col=np.arange(round((N/100.0)*(length)))\n",
    "    A[:, [col]]= amplitudes[:, [amp_col]]\n",
    "    P[:, [col]]= phases[:, [amp_col]]\n",
    "    UAP[:,[col]]=u_a[:,[amp_col]]\n",
    "    UAP[:,[col+length]]=u_pp[:,[amp_col]]\n",
    "    for m in range(10):\n",
    "        F,UF=AmpPhase2DFT(A[m,:], P[m,:], UAP[m,:])\n",
    "        X,UX=GUM_iDFT(F, UF)\n",
    "        x[m,:] = X\n",
    "        ux[m,:]=np.diag(UX)\n",
    "     \n",
    "    return x,ux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function execution\n",
    "x_time=[0]*len(sensor_train)\n",
    "ux_time=[0]*len(sensor_train)\n",
    "N=10 #percentage of amplitudes that were extracted from DFT results\n",
    "for i in range(len(sensor_train)):                     \n",
    "    print(\"Sensor number %s\" % i)\n",
    "    x_time[i],ux_time[i]=Reconstruct_time_domain(N,freq_of_sorted_values[i].values, sorted_values_amp_from_all_sensors[i].values,sorted_phases_from_all_sensors[i].values,sorted_uncer_from_all_sensors_a[i].values,sorted_uncer_from_all_sensors_ap[i].values,sorted_uncer_from_all_sensors_pp[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function execution\n",
    "x_time=[0]*len(sensor_train)\n",
    "ux_time=[0]*len(sensor_train)\n",
    "for i in range(len(sensor_train)):                     \n",
    "    print(\"Sensor number %s\" % i)\n",
    "    x_time[i],ux_time[i]=Reconstruct_time_domain_idft(N,freq_of_sorted_values[i].values, sorted_values_amp_from_all_sensors[i].values,sorted_phases_from_all_sensors[i].values,sorted_uncer_from_all_sensors_a[i].values,sorted_uncer_from_all_sensors_pp[i].values)\n",
    "       \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of time domain signal through all cycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib notebook\n",
    "units=['[Pa]', '[g]', '[g]', '[g]', '[kN]', '[bar]', '[mm/s]', '[A]', '[A]', '[A]', '[A]']\n",
    "labels1 = ['Microphone','Vibration plain bearing','Vibration piston rod','Vibration ball bearing', 'Axial force','Pressure','Velocity','Active current','Motor current phase 1','Motor current phase 2','Motor current phase 3']\n",
    "def plot_sensor(sensor,cycle):\n",
    "   \n",
    "    plt.figure(figsize=(15,12))\n",
    "    plt.plot(np.arange(0,1,0.0005),sensor_train[sensor].values.transpose()[cycle,:],label=\"Input time values\")\n",
    "    plt.ylabel(str(units[sensor]))\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "    plt.title(str(labels1[sensor]))\n",
    "    plt.errorbar(np.arange(0,1,0.0005),x_time[sensor][cycle],yerr=np.sqrt((ux_time[sensor][cycle])),label=\"Reconstructed time values with DFT\", ecolor='orangered',\n",
    "            color='green')\n",
    "    # Adding legend to the plot    \n",
    "        \n",
    "\n",
    "interact(plot_sensor, sensor=range(10),cycle=range(sorted_values_amp_from_all_sensors[0].shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "units=['[Pa]', '[g]', '[g]', '[g]', '[kN]', '[bar]', '[mm/s]', '[A]', '[A]', '[A]', '[A]']\n",
    "labels1 = ['Microphone','Vibration plain bearing','Vibration piston rod','Vibration ball bearing', 'Axial force','Pressure','Velocity','Active current','Motor current phase 1','Motor current phase 2','Motor current phase 3']\n",
    "def plot_sensor1(sensor,cycle):\n",
    "    plt.figure(figsize=(15,12))\n",
    "    plt.plot(np.arange(0,1,0.0005),sensor_train[sensor].values.transpose()[cycle,:], label=\"Input time values\")\n",
    "    plt.ylabel(str(units[sensor]))\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "    plt.title(str(labels1[sensor]))\n",
    "    plt.errorbar(np.arange(0,1,0.0005),x_time[sensor][cycle],yerr=np.sqrt((ux_time[sensor][cycle])),label=\"Reconstructed time values with DFT\", ecolor='orangered',\n",
    "            color='green')\n",
    "    # Adding legend to the plot    \n",
    "    plt.legend(loc='best', frameon=True)\n",
    "interact(plot_sensor1,sensor=range(10),cycle=widgets.IntSlider(min=0, max=sorted_values_amp_from_all_sensors[0].shape[0], step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test1.close()\n",
    "amp_fft1.close()\n",
    "freq_fft1.close()\n",
    "amp_dft2.close()\n",
    "freq_dft2.close()\n",
    "ph_dft2.close()\n",
    "u_a_dft2.close()\n",
    "u_ap_dft2.close()\n",
    "u_pp_dft2.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "[1]  PTB, ZeMA, - Deep dive into the ZeMA machine learning (ppt), January 2019\n",
    "\n",
    "[2]  https://www.nti-audio.com/en/support/know-how/fast-fourier-transform-fft\n",
    "\n",
    "[3]  http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r\n",
    "\n",
    "[4]  https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
    "\n",
    "[5]  Edouard Duchesnay, Tommy Löfstedt, - Statistics and Machine Learning in Python, March 2018\n",
    "\n",
    "[6]  https://ipywidgets.readthedocs.io/en/latest/examples/Using%20Interact.html\n",
    "\n",
    "[7]  Evaluation of measurement data — Supplement 1 to the “Guide to the expression of uncertainty in measurement” — Propagation      of distributions using a Monte Carlo method,  JCGM 101:2008 \n",
    "\n",
    "[8]  S Eichstädt (PTB) - Material for a Monte Carlo Uncertainty workshop with Jupyter notebooks https://github.com/eichstaedtPTB/MonteCarloHandsOn                   \n",
    "\n",
    "[9] S Eichstadt, A Link, P Harris and C. Elster - Efficient implementation of a Monte Carlo method for uncertainty evaluation in dynamic measurements, April 2012"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
